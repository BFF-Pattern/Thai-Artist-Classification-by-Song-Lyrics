{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1lgD7AnF5rv"
      },
      "source": [
        "# Install requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j4OEqEyGMx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3344af27-e5c2-4d8c-e969-2dd7ec2ad693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thai-Artist-Classification-by-Song-Lyrics'...\n",
            "remote: Enumerating objects: 1020, done.\u001b[K\n",
            "remote: Counting objects: 100% (306/306), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1020 (delta 186), reused 277 (delta 160), pack-reused 714\u001b[K\n",
            "Receiving objects: 100% (1020/1020), 156.98 MiB | 11.04 MiB/s, done.\n",
            "Resolving deltas: 100% (313/313), done.\n",
            "Checking out files: 100% (454/454), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch word2vec https://github.com/BFF-Pattern/Thai-Artist-Classification-by-Song-Lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df7kmAY9KBVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b495139f-5132-4d00-80b5-85b78af744b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 176 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 88.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 62.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 87.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 59.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 94.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.2 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60 kB 4.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.23.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.8 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.26.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.8->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.8->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.8->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q --upgrade torch torchvision\n",
        "!pip install -q pytorch_transformers\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q pyrouge\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip install -q pyrouge\n",
        "# !pyrouge_set_rouge_path \"/content/ThaiSum/BertSum/ROUGE-1.5.5\"\n",
        "# !apt update\n",
        "# !apt install -q libxml-parser-perl\n",
        "# %cd \"/content/ThaiSum/BertSum/ROUGE-1.5.5/data\"\n",
        "# !perl WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1J2XVx41wDvlcCeXHqmsSp1lSvwXtiy5i\n",
        "# !gdown 1-2-XV-x35DbDL4i5seXPk93aWtnDEg3q\n",
        "# !gdown 1v76sCx40VxufLl_RljFAlZVwoXDcd3T_\n",
        "!gdown 1vXgkV6_6hZyVzoKdTdtzkaLjv4BYQxz1\n",
        "# !gdown 1_QtoLQk0HpgxXdDsMCTmwVyXcEgHPW2c"
      ],
      "metadata": {
        "id": "_OUrv8MkZ9ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98813a37-6cd3-447b-bc33-91a2a71b3bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J2XVx41wDvlcCeXHqmsSp1lSvwXtiy5i\n",
            "To: /content/bert_only_weight.pt\n",
            "100% 711M/711M [00:04<00:00, 172MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vXgkV6_6hZyVzoKdTdtzkaLjv4BYQxz1\n",
            "To: /content/saved_weights.pt\n",
            "100% 713M/713M [00:03<00:00, 207MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/bert_only_weight.pt /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src\n",
        "!mv /content/saved_weights.pt /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src"
      ],
      "metadata": {
        "id": "cw3QPBcwgP2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CdVosLyeA1"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiSh3mH4ypEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b978328-c70d-4614-c1f2-ffb870a88159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src\n",
            "song.train.6.json\n",
            "[2022-05-26 17:18:49,670 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt not found in cache, downloading to /tmp/tmpx396sr7e\n",
            "100% 995526/995526 [00:00<00:00, 3123789.87B/s]\n",
            "[2022-05-26 17:18:50,275 INFO] copying /tmp/tmpx396sr7e to cache at /root/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "[2022-05-26 17:18:50,277 INFO] creating metadata file for /root/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "[2022-05-26 17:18:50,277 INFO] removing temp file /tmp/tmpx396sr7e\n",
            "[2022-05-26 17:18:50,277 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "[2022-05-26 17:18:50,401 INFO] Processing /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/js_data/song.train.6.json\n",
            "[2022-05-26 17:18:56,874 INFO] Saving to /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/bert_data/song.train.6.bert.pt\n",
            "song.test.6.json\n",
            "[2022-05-26 17:18:58,201 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "[2022-05-26 17:18:58,333 INFO] Processing /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/js_data/song.test.6.json\n",
            "[2022-05-26 17:19:00,014 INFO] Saving to /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/bert_data/song.test.6.bert.pt\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src' \n",
        "!python preprocess.py -mode format_to_bert -raw_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/js_data\" -save_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/bert_data\" -oracle_mode greedy -n_cpus 1 -log_file ../logs/preprocess.log"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View bert_data .pt file"
      ],
      "metadata": {
        "id": "APC_jT9hV1B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "pt_train = torch.load('/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/bert_data/song.train.6.bert.pt')\n",
        "df_train = pd.DataFrame(pt_train)\n",
        "# df_train\n",
        "\n",
        "pt_test = torch.load('/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/bert_data/song.test.6.bert.pt')\n",
        "df_test = pd.DataFrame(pt_test)\n",
        "# df_test"
      ],
      "metadata": {
        "id": "GicALHPRVHpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT6dxQTsnKqX",
        "outputId": "d430de6d-a50a-4b7b-9e34-16133af36f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['song_id', 'src', 'labels', 'segs', 'clss', 'src_txt'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_cls = 32"
      ],
      "metadata": {
        "id": "psyJaCW3ZiSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train['labels']<n_cls].reset_index()[df_train.columns]\n",
        "df_test = df_test[df_test['labels']<n_cls].reset_index()[df_test.columns]"
      ],
      "metadata": {
        "id": "wwQnH7-FmkI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mRTv9S8lTu1p",
        "outputId": "c1923a9e-2e03-4e18-d96c-defd4e37e5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   song_id                                                src  labels  \\\n",
              "0        4  [101, 1433, 53936, 22123, 37022, 43102, 20503,...      28   \n",
              "1        5  [101, 1436, 54633, 18427, 1400, 22598, 1426, 2...      29   \n",
              "2       10  [101, 46301, 1397, 111429, 16000, 80420, 1425,...      26   \n",
              "3       11  [101, 1454, 17405, 31904, 1450, 18427, 44334, ...       4   \n",
              "4       15  [101, 1451, 30011, 31904, 25915, 25915, 1417, ...       4   \n",
              "\n",
              "                                                segs clss  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0]   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0]   \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0]   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0]   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0]   \n",
              "\n",
              "                                             src_txt  \n",
              "0  [สุดท้าย ก็ เลิกกัน และ ฉัน ไม่ เหลือ ใคร, ครึ...  \n",
              "1  [อยาก จะ ยิ้ม ให้ เธอ ทุกที ที่ เจอกัน อยาก จะ...  \n",
              "2  [ใน คืนนี้ มี ดาว เป็น ล้าน ดวง แต่ ใจ ฉัน มี ...  \n",
              "3  [ไม่ เก่ง ไม่ กล้า พูด ความในใจ, ไม่ รู้ ว่า ม...  \n",
              "4  [แต่ คน คน นี้ ก็ ยัง รัก เธอ, ไม่ มี ผู้ใด เป...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c77dfdb-a117-462f-a006-bd9ccc34a117\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song_id</th>\n",
              "      <th>src</th>\n",
              "      <th>labels</th>\n",
              "      <th>segs</th>\n",
              "      <th>clss</th>\n",
              "      <th>src_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>[101, 1433, 53936, 22123, 37022, 43102, 20503,...</td>\n",
              "      <td>28</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[สุดท้าย ก็ เลิกกัน และ ฉัน ไม่ เหลือ ใคร, ครึ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>[101, 1436, 54633, 18427, 1400, 22598, 1426, 2...</td>\n",
              "      <td>29</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[อยาก จะ ยิ้ม ให้ เธอ ทุกที ที่ เจอกัน อยาก จะ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>[101, 46301, 1397, 111429, 16000, 80420, 1425,...</td>\n",
              "      <td>26</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[ใน คืนนี้ มี ดาว เป็น ล้าน ดวง แต่ ใจ ฉัน มี ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>[101, 1454, 17405, 31904, 1450, 18427, 44334, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[ไม่ เก่ง ไม่ กล้า พูด ความในใจ, ไม่ รู้ ว่า ม...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>[101, 1451, 30011, 31904, 25915, 25915, 1417, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[แต่ คน คน นี้ ก็ ยัง รัก เธอ, ไม่ มี ผู้ใด เป...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c77dfdb-a117-462f-a006-bd9ccc34a117')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c77dfdb-a117-462f-a006-bd9ccc34a117 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c77dfdb-a117-462f-a006-bd9ccc34a117');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_train, df_train['labels'], \n",
        "#                                                         random_state=2022, \n",
        "#                                                         test_size=0.3, \n",
        "#                                                         stratify=df_train['labels'])"
      ],
      "metadata": {
        "id": "yx4FiiQtpH8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetFF(Dataset):\n",
        "    def __init__(self, pt_df):\n",
        "      self.src = torch.tensor(pt_df['src'])\n",
        "      self.seg = torch.tensor(pt_df['segs'])\n",
        "      self.clss = torch.tensor(pt_df['clss'])\n",
        "      # self.mask = torch.tensor(pt_df['mask'])\n",
        "      self.y = torch.tensor(pt_df['labels'].tolist())\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        src = self.src[index]\n",
        "        seg = self.seg[index]\n",
        "        clss = self.clss[index]\n",
        "        # mask = self.mask[index]\n",
        "        y = self.y[index]\n",
        "\n",
        "        return src, seg, clss, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.src.shape[0]"
      ],
      "metadata": {
        "id": "LNB5cBq7dNMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_data = DatasetFF(df_train)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # wrap tensors\n",
        "# val_data = DatasetFF(df)\n",
        "\n",
        "# # sampler for sampling the data during training\n",
        "# val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# # dataLoader for train set\n",
        "# valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "test_data = DatasetFF(df_test)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "LRmYCGHjdZmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning BERT"
      ],
      "metadata": {
        "id": "3uvuFEkTZWxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel, BertConfig\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, temp_dir='temp', finetune=True):\n",
        "        super(Bert, self).__init__()\n",
        "        self.model = BertModel.from_pretrained('bert-base-multilingual-cased', cache_dir=temp_dir)\n",
        "\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def forward(self, x, segs):\n",
        "        if(self.finetune):\n",
        "            top_vec, _ = self.model(x, segs, attention_mask=None)\n",
        "        else:\n",
        "            self.eval()\n",
        "            with torch.no_grad():\n",
        "                top_vec, _ = self.model(x, segs, attention_mask=None)\n",
        "        return top_vec[-1]"
      ],
      "metadata": {
        "id": "2zB2bWMha-i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert, n_class=64):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768, 512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512, n_class)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, src, seg, clss):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(src, seg, attention_mask=None)\n",
        "\n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "mnr9HrINenzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1oaR3xgwKa-0D_ZGlIKhctdiVsUSMsi61\n",
        "!gdown 1TAlkMJIodCdBVCV6TH4ej23O8r465dF3\n",
        "!gdown 17_DtJNqJgoQJjPh2WxeOcWt3xEkcUGpG\n",
        "!gdown 1YAJv-ATlE2xt8PmBw7ZzCv_CjSxaC5iP\n",
        "!gdown 19bHRRCYGmWmMk-uIRTE9abDUIeZRehuO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQXVyuBIQWEM",
        "outputId": "cfa959f6-bebf-48b3-fd35-0b2a2e582b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oaR3xgwKa-0D_ZGlIKhctdiVsUSMsi61\n",
            "To: /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights_4.pt\n",
            "100% 713M/713M [00:06<00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TAlkMJIodCdBVCV6TH4ej23O8r465dF3\n",
            "To: /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights_8.pt\n",
            "100% 713M/713M [00:04<00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17_DtJNqJgoQJjPh2WxeOcWt3xEkcUGpG\n",
            "To: /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights_16.pt\n",
            "100% 713M/713M [00:06<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YAJv-ATlE2xt8PmBw7ZzCv_CjSxaC5iP\n",
            "To: /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights_32.pt\n",
            "100% 713M/713M [00:07<00:00, 99.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19bHRRCYGmWmMk-uIRTE9abDUIeZRehuO\n",
            "To: /content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights.pt\n",
            "100% 713M/713M [00:04<00:00, 153MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "bert = Bert()\n",
        "bert.load_state_dict(torch.load('/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/bert_only_weight.pt', map_location=lambda storage, loc: storage))\n"
      ],
      "metadata": {
        "id": "evOC61dsTC9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd62f04-f8af-4aff-cc1a-7c76730b9964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 662804195/662804195 [00:29<00:00, 22849637.29B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "05qn3SyI2ftP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# pass the pre-trained BERT to our define architecture, push the model to GPU\n",
        "model = BERT_Arch(bert.model, n_class=n_cls).to(device)\n",
        "model.load_state_dict(torch.load(f'/content/Thai-Artist-Classification-by-Song-Lyrics/models/newmodel/src/saved_weights_{n_cls}.pt', map_location=lambda storage, loc: storage))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23v2xB1XUhVe",
        "outputId": "1f6ddc1d-ed29-48c2-d43a-3b13557a734b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDnbkM1BlSML",
        "outputId": "4beb7c04-b943-41e0-bf53-03602e96115b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight ='balanced', classes= df_train['labels'].unique(), y = df_train['labels'].tolist())\n",
        "\n",
        "print(class_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw33CRQ1mfg-",
        "outputId": "f75a7be3-18e3-45e6-fdb5-ae86ad697d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.99888393 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393\n",
            " 0.99888393 0.99888393 1.03587963 0.99888393 0.99888393 0.99888393\n",
            " 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393\n",
            " 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393\n",
            " 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393 0.99888393\n",
            " 0.99888393 0.99888393]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts, dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "jqphwCFOm5vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "VytPBt8sffKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Train & Evaluation function"
      ],
      "metadata": {
        "id": "JvKuEnJflJWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # print(f\"batch: {batch}\")\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    src, seg, clss, labels = batch\n",
        "\n",
        "    # print(src)\n",
        "\n",
        "    # src = src.to(device)\n",
        "    # seg = seg.to(device)\n",
        "    # clss = clss.to(device)\n",
        "    # labels = labels.to(device)\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(src, seg, clss)\n",
        "\n",
        "    # print(preds.shape)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "Sjj4toK9jlYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # function for evaluating the model\n",
        "# def evaluate():\n",
        "  \n",
        "#   print(\"\\nEvaluating...\")\n",
        "  \n",
        "#   # deactivate dropout layers\n",
        "#   model.eval()\n",
        "\n",
        "#   total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "#   # empty list to save the model predictions\n",
        "#   total_preds = []\n",
        "\n",
        "#   # iterate over batches\n",
        "#   for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "#     # Progress update every 50 batches.\n",
        "#     if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "#       # Calculate elapsed time in minutes.\n",
        "#       elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "#       # Report progress.\n",
        "#       print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "#     # push the batch to gpu\n",
        "#     batch = [t.to(device) for t in batch]\n",
        "\n",
        "#     sent_id, mask, labels = batch\n",
        "\n",
        "#     # deactivate autograd\n",
        "#     with torch.no_grad():\n",
        "      \n",
        "#       # model predictions\n",
        "#       preds = model(sent_id, mask)\n",
        "\n",
        "#       # compute the validation loss between actual and predicted values\n",
        "#       loss = cross_entropy(preds,labels)\n",
        "\n",
        "#       total_loss = total_loss + loss.item()\n",
        "\n",
        "#       preds = preds.detach().cpu().numpy()\n",
        "\n",
        "#       total_preds.append(preds)\n",
        "\n",
        "#   # compute the validation loss of the epoch\n",
        "#   avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "#   # reshape the predictions in form of (number of samples, no. of classes)\n",
        "#   total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "#   return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "ut46J3FwoJhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hrs0_ouJZryH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Model Train"
      ],
      "metadata": {
        "id": "aOlYOmCqoWgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    # valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    # valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    # print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhwldLhHn8Uj",
        "outputId": "658c588f-562e-4dd8-84e9-124576de6794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 3.467\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 3.467\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 3.467\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 3.467\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 3.467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'/content/saved_weights_{n_cls}.pt')"
      ],
      "metadata": {
        "id": "_dqleVGovwVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calc_accuracy(top_n=[1,2,4,8,16,32,64]):\n",
        "top_n = [1,2,4,8,16,32]\n",
        "model.eval()\n",
        "\n",
        "total_accuracy1, total_accuracy2, total_accuracy4, total_accuracy8, total_accuracy16, total_accuracy32 = (0,0,0,0,0,0)\n",
        "\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "  # Progress update every 50 batches.\n",
        "  if step % 50 == 0 and not step == 0:\n",
        "    # Report progress.\n",
        "    print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\n",
        "  \n",
        "  batch = [t.to(device) for t in batch]\n",
        "\n",
        "  src, seg, clss, labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # model predictions\n",
        "    preds = model(src, seg, clss)\n",
        "\n",
        "    # sort topk\n",
        "    sorted_preds = torch.argsort(preds, dim=1)\n",
        "    ranked_preds = sorted_preds[torch.arange(sorted_preds.size(0)), labels]\n",
        "\n",
        "    total_accuracy1 += (ranked_preds < top_n[0]).sum()\n",
        "    total_accuracy2 += (ranked_preds < top_n[1]).sum()\n",
        "    total_accuracy4 += (ranked_preds < top_n[2]).sum()\n",
        "    total_accuracy8 += (ranked_preds < top_n[3]).sum()\n",
        "    total_accuracy16 += (ranked_preds < top_n[4]).sum()\n",
        "    total_accuracy32 += (ranked_preds < top_n[5]).sum()\n",
        "\n",
        "avg_accuracy1 = total_accuracy1 / len(test_data)\n",
        "avg_accuracy2 = total_accuracy2 / len(test_data)\n",
        "avg_accuracy4 = total_accuracy4 / len(test_data)\n",
        "avg_accuracy8 = total_accuracy8 / len(test_data)\n",
        "avg_accuracy16 = total_accuracy16 / len(test_data)\n",
        "avg_accuracy32 = total_accuracy32 / len(test_data)"
      ],
      "metadata": {
        "id": "QQv8ppCExRrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd148b97-e8a4-4ca5-f515-d3129afa0080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     56.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_accuracy1, avg_accuracy2, avg_accuracy4, avg_accuracy8, avg_accuracy16, avg_accuracy32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nTyUI2cNa8",
        "outputId": "2bb6134f-1228-4d01-b01d-d3e9177ea7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0314, device='cuda:0'),\n",
              " tensor(0.0628, device='cuda:0'),\n",
              " tensor(0.1256, device='cuda:0'),\n",
              " tensor(0.2511, device='cuda:0'),\n",
              " tensor(0.5022, device='cuda:0'),\n",
              " tensor(1., device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b_K3hQqzWqN"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80VDsXvGzbPk"
      },
      "outputs": [],
      "source": [
        "# # Train (ARedSum-Base) Train a Salience Ranker\n",
        "# !python train.py -bert_data_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -visible_gpus 0 -gpu_ranks 0 -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/results/newmodel_base\" -model_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/model_checkpoint/newmodel_base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrmnl_1A06MD"
      },
      "outputs": [],
      "source": [
        "# # Train (ARedSum-CTX) Train a Ranker for Selection\n",
        "# !python train.py -fix_scorer -train_from /path/to/the/best/salience/ranker.pt -bert_data_path /path/to/cnndm_or_nyt50/bert_data/ -visible_gpus 2 -gpu_ranks 0 -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -model_name ctx -max_epoch 2 -train_steps 50000 -label_format soft -use_rouge_label t -valid_by_rouge t -rand_input_thre 1.0 -temperature 20 -seg_count 30 -ngram_seg_count 20,20,20 -bilinear_out 20 -result_path /path/to/where/you/want/to/save/the/preidicted/summaries -model_path /path/to/where/you/want/to/save/the/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXQR2ZQ20ya1"
      },
      "outputs": [],
      "source": [
        "# # Train (ARedSum-SEQ) Train a Sequence Generation Model\n",
        "# !python train.py -bert_data_path /path/to/cnndm_or_nyt50/bert_data/ -visible_gpus 2 -gpu_ranks 0 -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -model_name seq -max_epoch 2 -train_steps 50000 -label_format soft -use_rouge_label t -valid_by_rouge t -rand_input_thre 0.8 -temperature 20 -result_path /path/to/where/you/want/to/save/the/preidicted/summaries -model_path /path/to/where/you/want/to/save/the/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH5e91Bu2Srv"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziscOITR2bGc"
      },
      "source": [
        "## Evaluate by ROUGE Score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Rw8-Qu2dJp",
        "outputId": "9f5ca98a-f3f2-42f9-a991-38e69c2da7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2020-11-14 08:30:40,819 INFO] Loading checkpoint from /content/ThaiSum/ARedSum/model_checkpoint/ARedSum_base.pt\n",
            "Namespace(accum_count=2, aggr='last', batch_size=3000, bert_config_path='config/bert_config_uncased_base.json', bert_data_path='/content/ThaiSum/ARedSum/bert_data/thaisum', beta1=0.9, beta2=0.999, bilinear_out=10, block_trigram=True, dataset='', decay_method='noam', dropout=0.1, ff_size=2048, fix_scorer=False, gpu_ranks=[0], heads=8, hidden_size=128, inter_layers=2, label_format='soft', log_file='../logs/cnndm.log', loss='wsoftmax', lr=0.002, max_epoch=2, max_grad_norm=0, max_label_sent_count=3, mode='test', model_name='base', model_path='../models/', ngram_seg_count='20,20,20', optim='adam', param_init=0, param_init_glorot=True, rand_input_thre=1.0, recall_eval=False, report_every=50, report_precision=True, report_rouge=True, result_path='/content/ThaiSum/ARedSum/results/aredsum_base', rnn_size=512, salience_softmax=False, save_checkpoint_steps=2000, save_model_count=3, seed=666, seg_count=30, sent_sel_method='truth', temp_dir='../temp', temperature=20, test_all=False, test_from='/content/ThaiSum/ARedSum/model_checkpoint/ARedSum_base.pt', train_from='', train_steps=50000, use_doc=False, use_interval=True, use_rouge_label=False, valid_by_rouge=False, visible_gpus='0', warmup_steps=10000, world_size=1)\n",
            "gpu_rank 0\n",
            "[2020-11-14 08:30:49,653 INFO] * number of parameters: 189473537\n",
            "[2020-11-14 08:30:49,963 INFO] Loading test dataset from /content/ThaiSum/ARedSum/bert_data/thaisum.test.1.bert.pt, number of examples: 1000\n",
            "[2020-11-14 08:30:49,963 INFO] input_batch_size:3000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2020-11-14 08:31:35,664 INFO] [PERF]At step 0: rouge1:54.06 rouge2:48.22\n",
            "[2020-11-14 08:31:35,664 INFO] [PERF]MacroPrecision at step 0: P@1:39.90%\tP@2:28.45%\tP@3:18.97%\n",
            "[2020-11-14 08:31:35,664 INFO] [PERF]MicroPrecision at step 0: P@1:39.90%\tP@2:28.45%\tP@3:18.97%\n",
            "1000\n",
            "1000\n",
            "2020-11-14 08:31:35,750 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2020-11-14 08:31:35,750 INFO] Writing summaries.\n",
            "2020-11-14 08:31:35,750 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpxzy_twje/system and model files to ../temp/tmpxzy_twje/model.\n",
            "[2020-11-14 08:31:35,750 INFO] Processing summaries. Saving system files to ../temp/tmpxzy_twje/system and model files to ../temp/tmpxzy_twje/model.\n",
            "2020-11-14 08:31:35,750 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2020-11-14-08-31-35/candidate/.\n",
            "[2020-11-14 08:31:35,750 INFO] Processing files in ../temp/rouge-tmp-2020-11-14-08-31-35/candidate/.\n",
            "2020-11-14 08:31:35,839 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpxzy_twje/system.\n",
            "[2020-11-14 08:31:35,839 INFO] Saved processed files to ../temp/tmpxzy_twje/system.\n",
            "2020-11-14 08:31:35,840 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2020-11-14-08-31-35/reference/.\n",
            "[2020-11-14 08:31:35,840 INFO] Processing files in ../temp/rouge-tmp-2020-11-14-08-31-35/reference/.\n",
            "2020-11-14 08:31:35,923 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpxzy_twje/model.\n",
            "[2020-11-14 08:31:35,923 INFO] Saved processed files to ../temp/tmpxzy_twje/model.\n",
            "2020-11-14 08:31:36,598 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpdqq8tos0/rouge_conf.xml\n",
            "[2020-11-14 08:31:36,598 INFO] Written ROUGE configuration to ../temp/tmpdqq8tos0/rouge_conf.xml\n",
            "[2020-11-14 08:31:40,328 INFO] [PERF]Rouges at step 0: RG1-P:42.77\tRG1-R:56.72\tRG1-F:45.16\tRG2-P:20.05\tRG2-R:29.11\tRG2-F:21.83\tRGL-P:42.71\tRGL-R:56.59\tRGL-F:45.08 \n",
            "\n",
            "[2020-11-14 08:31:40,335 INFO] Validation xent: 1.08809 at step 0\n"
          ]
        }
      ],
      "source": [
        "# Evaluate ARedSum-Base \n",
        "!python train.py -bert_data_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -visible_gpus 0 -gpu_ranks 0 -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode test -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/results/aredsum_base\" -test_from \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/model_checkpoint/ARedSum_base.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iOoXUG54pEu"
      },
      "source": [
        "\n",
        "ROUGE Scores are shown here\n",
        "```\n",
        "[2020-11-14 08:31:40,328 INFO] [PERF]Rouges at step 0: RG1-P:42.77\tRG1-R:56.72\tRG1-F:45.16\tRG2-P:20.05\tRG2-R:29.11\tRG2-F:21.83\tRGL-P:42.71\tRGL-R:56.59\tRGL-F:45.08 \n",
        "```\n",
        "Therefore ROUGE-F1 results are:  R1=45.16, R2=21.83, RL=45.08.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAlOcVwj5KNO"
      },
      "source": [
        "## Evaluate by BertScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l66k5RuG5Vjh"
      },
      "outputs": [],
      "source": [
        "!pip install -q bert_score\n",
        "import bert_score\n",
        "from bert_score import score\n",
        "import logging\n",
        "import transformers\n",
        "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
        "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
        "transformers.modeling_utils.logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrUrZeP5aq4"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/results/aredsum_base_step0_initial.candidate\") as f: # Output Summary \n",
        "    cands = [line.strip() for line in f]\n",
        "\n",
        "with open(\"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/bert_data/thaisum\" -accum_count 2 -report_every 50 -save_checkpoint_steps 2000 -decay_method noam -mode train -model_name base -label_format soft -result_path \"/content/Thai-Artist-Classification-by-Song-Lyrics/newmodel/results/aredsum_base_step0_initial.gold\") as f:  # Reference Summary\n",
        "    refs = [line.strip() for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwB6iIJz5h7G",
        "outputId": "eb1851f0-7051-4741-8599-f55086ede857"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System level F1 score: 81.076\n",
            "System level P score: 79.806\n",
            "System level R score: 82.700\n"
          ]
        }
      ],
      "source": [
        "P, R, F1 = score(cands, refs, lang='th', verbose=False)\n",
        "\n",
        "print(f\"System level F1 score: {F1.mean()*100:.3f}\") ##  *100 to make it simplier to read similar to ROUGE.\n",
        "print(f\"System level P score: {P.mean()*100:.3f}\")\n",
        "print(f\"System level R score: {R.mean()*100:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "8La9mENr6BU3",
        "outputId": "ae553ed5-1379-4ab5-a9de-850ef188894d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(F1, bins=20)\n",
        "plt.xlabel(\"score\")\n",
        "plt.ylabel(\"counts\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WH5e91Bu2Srv"
      ],
      "machine_shape": "hm",
      "name": "Copy of (NATH) NewModel_for_Thai_text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}